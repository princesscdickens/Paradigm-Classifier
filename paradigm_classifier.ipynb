{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nov 19 2019\n",
    "\n",
    "# Classifiers with scikit\n",
    "\n",
    "In this homework, the task is to build a classifier that learns the conjugation/declension types of verbs and nouns, given examples. The focus is on designing a good set of linguistically informed features to do this. What classifier you use is up to you. Any of the standard ones will work: Naive Bayes, perceptron, SVM, logistic regression (MaxEnt), or k-NN, or even decision lists. You are free to use the classifiers in scikit_learn or take advantage of the ones you've already built for other homeworks. You may use a kernelized classifier if you want to, or a linear classifier, the choice is up to you. A simple straightforward choice would be the linear SVM, available as LinearSVC in scikit.\n",
    "\n",
    "Classification\n",
    "\n",
    "The task consists of basically three parts:\n",
    "\n",
    "(1) Split the data into at least train/test if not train/dev/test (should you need a dev set) in some reasonable way (90/10 or 80/10/10). You are not given a split and should design and implement this random split yourself. You really want to randomize this because the first classes 0,1,2 ... are very big, and the last ones very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 'vomitar'), ('0', 'eructar'), ('0', 'tantear')]\n",
      "[('0', 'empachar'), ('4', 'redimir'), ('12', 'acentuar')]\n",
      "Training set German nouns: 2051\n",
      "Test set German nouns: 513\n",
      "Proportion test/training German nouns: 0.250121891760117\n"
     ]
    }
   ],
   "source": [
    "# Data consists of:\n",
    "# * German nouns (de_n.txt), German verbs (de_v.txt)\n",
    "# * Finnish nouns/adjectives (fi_na.txt), Finnish verbs (fi_v.txt)\n",
    "# * Spanish verbs (es_v.txt)\n",
    "\n",
    "# I will divide each into a training set (80%) and a test set (20%)\n",
    "# X = features, y = classes\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm \n",
    "import random\n",
    "\n",
    "# Read in data 'y\\tword' and convert to [(y, word1),(y, word2),...]\n",
    "def makeLists(fileName):\n",
    "    lines = [line.strip() for line in open(fileName)]\n",
    "    list1 = []\n",
    "    for line in lines:\n",
    "        values = line.split('\\t')\n",
    "        list1.append((values[0],values[1]))\n",
    "    return list1\n",
    "\n",
    "values_de_n = makeLists('de_n.txt') # German nouns (de_n.txt) \n",
    "values_de_v = makeLists('de_v.txt') # German verbs (de_v.txt) \n",
    "values_fi_na = makeLists('fi_na.txt') # Finnish nouns/adjectives (fi_na.txt) \n",
    "values_fi_v = makeLists('fi_v.txt') # Finnish verbs (fi_v.txt) \n",
    "values_es_v = makeLists('es_v.txt') # Spanish verbs (es_v.txt) \n",
    "print(values_es_v[0:3]) \n",
    "\n",
    "# The number of conjugation/declension classes are as follows:\n",
    "#de_n: 70\n",
    "#de_v: 140\n",
    "#fi_na: 258\n",
    "#fi_v: 282\n",
    "#es_v: 97\n",
    "\n",
    "# split data randomly \n",
    "random.shuffle(values_de_n)\n",
    "random.shuffle(values_de_v)\n",
    "random.shuffle(values_fi_na)\n",
    "random.shuffle(values_fi_v)\n",
    "random.shuffle(values_es_v)\n",
    "print(values_es_v[0:3]) # check if shuffled\n",
    "\n",
    "def splitData(inputList, percentTest):\n",
    "    testNum = round(len(inputList) * percentTest)\n",
    "    trainNum = len(inputList) - testNum\n",
    "    trainingData = inputList[0:trainNum]\n",
    "    testData = inputList[trainNum:]\n",
    "    return trainingData, testData\n",
    "\n",
    "de_n_train, de_n_test = splitData(values_de_n, 0.2)\n",
    "de_v_train, de_v_test = splitData(values_de_v, 0.2)\n",
    "fi_na_train, fi_na_test = splitData(values_fi_na, 0.2)\n",
    "fi_v_train, fi_v_test = splitData(values_fi_v, 0.2)\n",
    "es_v_train, es_v_test = splitData(values_es_v, 0.2)\n",
    "\n",
    "print('Training set German nouns:', len(de_n_train))\n",
    "print('Test set German nouns:',len(de_n_test))\n",
    "print('Proportion test/training German nouns:', len(de_n_test) / len(de_n_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Convert each word into a feature representation of your design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: [('7', {'pfx1=K': 1, 'pfx2=Kl': 1, 'pfx3=Kle': 1, 'pfx4=Klem': 1, 'sfx1=n': 1, 'sfx2=in': 1, 'sfx3=rin': 1, 'sfx4=erin': 1, 'hasa=no': 1, 'hase=yes': 1, 'hasi=yes': 1, 'haso=no': 1, 'hasu=no': 1, 'hasy=no': 1, 'hasä=no': 1, 'hasö=no': 1, 'hasü=no': 1, 'Kl': 1, 'le': 1, 'em': 1, 'mp': 1, 'pn': 1, 'ne': 1, 'er': 1, 'ri': 1, 'in': 1, 'Kle': 1, 'lem': 1, 'emp': 1, 'mpn': 1, 'pne': 1, 'ner': 1, 'eri': 1, 'rin': 1}), ('1', {'pfx1=B': 1, 'pfx2=Bu': 1, 'pfx3=Buc': 1, 'pfx4=Buch': 1, 'sfx1=e': 1, 'sfx2=he': 1, 'sfx3=che': 1, 'sfx4=uche': 1, 'hasa=no': 1, 'hase=yes': 1, 'hasi=no': 1, 'haso=no': 1, 'hasu=yes': 1, 'hasy=no': 1, 'hasä=no': 1, 'hasö=no': 1, 'hasü=no': 1, 'Bu': 1, 'uc': 1, 'ch': 1, 'he': 1, 'Buc': 1, 'uch': 1, 'che': 1}), ('8', {'pfx1=S': 1, 'pfx2=So': 1, 'pfx3=Soz': 1, 'pfx4=Sozi': 1, 'sfx1=e': 1, 'sfx2=ge': 1, 'sfx3=oge': 1, 'sfx4=loge': 1, 'hasa=no': 1, 'hase=yes': 1, 'hasi=yes': 1, 'haso=yes': 1, 'hasu=no': 1, 'hasy=no': 1, 'hasä=no': 1, 'hasö=no': 1, 'hasü=no': 1, 'So': 1, 'oz': 1, 'zi': 1, 'io': 1, 'ol': 1, 'lo': 1, 'og': 1, 'ge': 1, 'Soz': 1, 'ozi': 1, 'zio': 1, 'iol': 1, 'olo': 1, 'log': 1, 'oge': 1})]\n",
      "Number tuples: 2051\n",
      "Size each dict: 34\n",
      "Total features: 69734\n"
     ]
    }
   ],
   "source": [
    "# Features: word itself, prefixes(4), suffixes(4), vowels\n",
    "\n",
    "def hasVowel(inputVowel, inputWord):\n",
    "    if inputVowel in inputWord:\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "\n",
    "def extractFeatures(inputWord):\n",
    "    featuresVec = []\n",
    "    bigrams = [b[0]+b[1] for b in zip(inputWord,inputWord[1:])]\n",
    "    trigrams = [t[0]+t[1]+t[2] for t in zip(inputWord,inputWord[1:], inputWord[2:])]\n",
    "    f1 = 'pfx1=' + inputWord[0] #prefix\n",
    "    f2 = 'pfx2=' + inputWord[0:2]\n",
    "    f3 = 'pfx3=' + inputWord[0:3]\n",
    "    f4 = 'pfx4=' + inputWord[0:4]\n",
    "    f5 = 'sfx1=' + inputWord[-1] #suffix\n",
    "    f6 = 'sfx2=' + inputWord[-2:]\n",
    "    f7 = 'sfx3=' + inputWord[-3:]\n",
    "    f8 = 'sfx4=' + inputWord[-4:]\n",
    "    f9 = 'hasa=' + hasVowel('a', inputWord) # check vowel inventory !!\n",
    "    f10 = 'hase=' + hasVowel('e', inputWord)\n",
    "    f11 = 'hasi=' + hasVowel('i', inputWord)\n",
    "    f12 = 'haso=' + hasVowel('o', inputWord)\n",
    "    f13 = 'hasu=' + hasVowel('u', inputWord)\n",
    "    f14 = 'hasy=' + hasVowel('y', inputWord)\n",
    "    f15 = 'hasä=' + hasVowel('ä', inputWord)\n",
    "    f16 = 'hasö=' + hasVowel('ö', inputWord)\n",
    "    f17 = 'hasü=' + hasVowel('ü', inputWord)\n",
    "    featuresVec = [f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17] + bigrams + trigrams\n",
    "    return featuresVec\n",
    "\n",
    "#test1 = extractFeatures('oír')\n",
    "#print(test1)\n",
    "\n",
    "# I now have a training set (~80%) and a test set (~20%) for each of the 5 groups in format [(class, word)]\n",
    "# AND a function for generating features: extractFeatures(inputWord)\n",
    "# de_n_train, de_n_test \n",
    "# de_v_train, de_v_test\n",
    "# fi_na_train, fi_na_test \n",
    "# fi_v_train, fi_v_test \n",
    "# es_v_train, es_v_test \n",
    "\n",
    "def expandFeaturesList(inputList):\n",
    "    newList = []\n",
    "    for item in inputList: # [(class, word)]\n",
    "        dict1 = {}\n",
    "        class1 = item[0]\n",
    "        newFeatures = extractFeatures(item[1])\n",
    "        for feature in newFeatures:\n",
    "            dict1[feature] = 1\n",
    "        newList.append((class1, dict1)) # [(class1, {feature: 1}), (class2, {feature: 1})]\n",
    "    return newList # list of dictionaries {feature:class}\n",
    "\n",
    "de_n_train_expanded = expandFeaturesList(de_n_train) # German nouns\n",
    "print('Training set:', de_n_train_expanded[0:3])\n",
    "print('Number tuples:', len(de_n_train_expanded))\n",
    "print('Size each dict:', len(de_n_train_expanded[0][1]))\n",
    "print('Total features:', len(de_n_train_expanded) * len(de_n_train_expanded[0][1]))\n",
    "\n",
    "de_v_train_expanded = expandFeaturesList(de_v_train) # German verbs\n",
    "fi_na_train_expanded = expandFeaturesList(fi_na_train) # Finnish nouns + adj\n",
    "fi_v_train_expanded = expandFeaturesList(fi_v_train) # Finnish verbs\n",
    "es_v_train_expanded = expandFeaturesList(es_v_train) # Spanish verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features should have a value of 1 in the dictionary.\n",
    "The classes are kept in a separate vector\n",
    "\n",
    "For evaluation, the argument should be a word (a string) and inside the function, that word can be expanded with the features I defined above. Then I can predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['1', '0', '37', '0', '7', '0', '4', '4', '0', '4']\n",
      "Len classes vector: 2051\n",
      "Len features vector: 2051\n",
      "Features[0]: {'pfx1=K': 1, 'pfx2=Kl': 1, 'pfx3=Kle': 1, 'pfx4=Klem': 1, 'sfx1=n': 1, 'sfx2=in': 1, 'sfx3=rin': 1, 'sfx4=erin': 1, 'hasa=no': 1, 'hase=yes': 1, 'hasi=yes': 1, 'haso=no': 1, 'hasu=no': 1, 'hasy=no': 1, 'hasä=no': 1, 'hasö=no': 1, 'hasü=no': 1, 'Kl': 1, 'le': 1, 'em': 1, 'mp': 1, 'pn': 1, 'ne': 1, 'er': 1, 'ri': 1, 'in': 1, 'Kle': 1, 'lem': 1, 'emp': 1, 'mpn': 1, 'pne': 1, 'ner': 1, 'eri': 1, 'rin': 1}\n",
      "Features[1]: {'pfx1=B': 1, 'pfx2=Bu': 1, 'pfx3=Buc': 1, 'pfx4=Buch': 1, 'sfx1=e': 1, 'sfx2=he': 1, 'sfx3=che': 1, 'sfx4=uche': 1, 'hasa=no': 1, 'hase=yes': 1, 'hasi=no': 1, 'haso=no': 1, 'hasu=yes': 1, 'hasy=no': 1, 'hasä=no': 1, 'hasö=no': 1, 'hasü=no': 1, 'Bu': 1, 'uc': 1, 'ch': 1, 'he': 1, 'Buc': 1, 'uch': 1, 'che': 1}\n"
     ]
    }
   ],
   "source": [
    "# Now to create X and y vectors for training for *German nouns*\n",
    "de_n_y_train = [item[0] for item in de_n_train_expanded] #classes\n",
    "de_n_X_train = [item[1] for item in de_n_train_expanded] #features present [{f1:1,f2:1},{f1:1,f2:1}]\n",
    "print('Classes:', de_n_y_train[30:40])\n",
    "print('Len classes vector:', len(de_n_y_train))\n",
    "print('Len features vector:', len(de_n_X_train))\n",
    "print('Features[0]:', de_n_X_train[0])\n",
    "print('Features[1]:', de_n_X_train[1])\n",
    "\n",
    "# Each feature gets an index --> I should fit() and transform() the X vector separately, \n",
    "# so I can use the vectorizer later for prediction (otherwise it'll say I don't have enough features for one instance)\n",
    "de_n_vectorizer = DictVectorizer(sparse = True).fit(de_n_X_train)\n",
    "de_n_X = de_n_vectorizer.transform(de_n_X_train)\n",
    "#print('After vector transformation for de_n:\\n', de_n_X[0])\n",
    "\n",
    "# German verbs\n",
    "de_v_y_train = [item[0] for item in de_v_train_expanded] #classes\n",
    "de_v_X_train = [item[1] for item in de_v_train_expanded]\n",
    "de_v_vectorizer = DictVectorizer(sparse = True).fit(de_v_X_train)\n",
    "de_v_X = de_v_vectorizer.transform(de_v_X_train) # de_v_X and de_v_y_train\n",
    "\n",
    "# Finnish nouns + adj\n",
    "fi_na_y_train = [item[0] for item in fi_na_train_expanded] #classes\n",
    "fi_na_X_train = [item[1] for item in fi_na_train_expanded]\n",
    "fi_na_vectorizer = DictVectorizer(sparse = True).fit(fi_na_X_train)\n",
    "fi_na_X = fi_na_vectorizer.transform(fi_na_X_train) # fi_na_X and fi_na_y_train\n",
    "\n",
    "# Finnish verbs\n",
    "fi_v_y_train = [item[0] for item in fi_v_train_expanded] #classes\n",
    "fi_v_X_train = [item[1] for item in fi_v_train_expanded]\n",
    "fi_v_vectorizer = DictVectorizer(sparse = True).fit(fi_v_X_train)\n",
    "fi_v_X = fi_v_vectorizer.transform(fi_v_X_train) # fi_v_X and fi_v_y_train\n",
    "\n",
    "# Spanish verbs\n",
    "es_v_y_train = [item[0] for item in es_v_train_expanded] #classes\n",
    "es_v_X_train = [item[1] for item in es_v_train_expanded]\n",
    "es_v_vectorizer = DictVectorizer(sparse = True).fit(es_v_X_train)\n",
    "es_v_X = es_v_vectorizer.transform(es_v_X_train) # es_v_X and es_v_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Train a classifier and evaluate its performance (accuracy) using this feature representation. Report accuracies for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier --> clf.fit(X, y) *German nouns*\n",
    "de_n_clf = svm.LinearSVC() # use Linear SVM\n",
    "de_n_clf.fit(de_n_X, de_n_y_train)\n",
    "\n",
    "# German verbs\n",
    "de_v_clf = svm.LinearSVC() # use Linear SVM\n",
    "de_v_clf.fit(de_v_X, de_v_y_train)\n",
    "\n",
    "# Finnish nouns + adj\n",
    "fi_na_clf = svm.LinearSVC() # use Linear SVM\n",
    "fi_na_clf.fit(fi_na_X, fi_na_y_train)\n",
    "\n",
    "# Finnish verbs\n",
    "fi_v_clf = svm.LinearSVC() # use Linear SVM\n",
    "fi_v_clf.fit(fi_v_X, fi_v_y_train)\n",
    "\n",
    "# Spanish verbs\n",
    "es_v_clf = svm.LinearSVC() # use Linear SVM\n",
    "es_v_clf.fit(es_v_X, es_v_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to evaluate:\n",
    "- This function accepts a list of tuples (class, word) as an argument\n",
    "1. Take the word and pass it to extractFeatures(word)\n",
    "2. Create y and X vectors for the word\n",
    "3. Pass the X vector through vectorizer\n",
    "   - vectorizer1 = DictVectorizer(sparse = True)\n",
    "   - X = vectorizer1.fit_transform(inputList)\n",
    "4. Then pass the vectorized X to clf.predict(de_n_vectorizer2.transform(inputList))\n",
    "5. Check if prediction matches gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number correct: 375\n",
      "Total guesses: 513\n",
      "German noun accuracy: 0.7309941520467836 \n",
      "\n",
      "Number correct: 294\n",
      "Total guesses: 365\n",
      "German verb accuracy: 0.8054794520547945 \n",
      "\n",
      "Number correct: 723\n",
      "Total guesses: 771\n",
      "Spanish verb accuracy: 0.9377431906614786 \n",
      "\n",
      "Number correct: 972\n",
      "Total guesses: 1200\n",
      "Finnish noun and adjective accuracy: 0.81 \n",
      "\n",
      "Number correct: 1343\n",
      "Total guesses: 1410\n",
      "Finnish verb accuracy: 0.9524822695035461 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test set looks like: [(class, word), (class, word),...]\n",
    "# I now have a trained classifier for each category\n",
    "# de_n_test --> de_n_clf\n",
    "# de_v_test --> de_v_clf\n",
    "# fi_na_test --> fi_na_clf\n",
    "# fi_v_test --> fi_v_clf\n",
    "# es_v_test --> es_v_clf\n",
    "\n",
    "def evaluate(inputListTuples, inputTrainedClassifier, vectorizer1): # input [(class, word), (class, word),...]\n",
    "    numIncorrect = 0\n",
    "    numTotal = len(inputListTuples)\n",
    "    \n",
    "    for element in inputListTuples: #(class, word)\n",
    "        y = element[0]\n",
    "        featuresList = extractFeatures(element[1]) #returns ['feature1','feature2', ...]\n",
    "        featuresDict = {string:1 for string in featuresList} #returns {feature1:1, feature2:1, ...}\n",
    "        # print('Features Dict:', featuresDict) #{'word=Schmierfett': 1, 'pfx1=S': 1,...}\n",
    "        \n",
    "        # transform features vec with vectorizer that has been previously fitted\n",
    "        X = vectorizer1.transform(featuresDict)\n",
    "        #print(X) #This seems to work\n",
    "        \n",
    "        #prediction\n",
    "        guess = inputTrainedClassifier.predict(X)\n",
    "        \n",
    "        if guess != y:\n",
    "            numIncorrect += 1\n",
    "    \n",
    "    totalCorrect = numTotal - numIncorrect\n",
    "    print(\"Number correct:\", totalCorrect)\n",
    "    print(\"Total guesses:\", numTotal)\n",
    "    accuracy = totalCorrect/numTotal\n",
    "    return accuracy\n",
    "\n",
    "#print('InputXVec:',de_n_test[0:3]) #[('0', 'Schmierfett'), ('1', 'Agoraphobie'), ('2', 'Mechaniker')]\n",
    "#print('InputClassifier:', de_n_clf) #this works\n",
    "\n",
    "de_n_accuracy = evaluate(de_n_test, de_n_clf, de_n_vectorizer)\n",
    "print('German noun accuracy:', de_n_accuracy, '\\n')\n",
    "\n",
    "de_v_accuracy = evaluate(de_v_test, de_v_clf, de_v_vectorizer)\n",
    "print('German verb accuracy:', de_v_accuracy, '\\n')\n",
    "\n",
    "es_v_accuracy = evaluate(es_v_test, es_v_clf, es_v_vectorizer)\n",
    "print('Spanish verb accuracy:', es_v_accuracy, '\\n')\n",
    "\n",
    "fi_na_accuracy = evaluate(fi_na_test, fi_na_clf, fi_na_vectorizer)\n",
    "print('Finnish noun and adjective accuracy:', fi_na_accuracy, '\\n')\n",
    "\n",
    "fi_v_accuracy = evaluate(fi_v_test, fi_v_clf, fi_v_vectorizer)\n",
    "print('Finnish verb accuracy:', fi_v_accuracy, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to hand in\n",
    "\n",
    "You should hand in a Python/Jupyter file that works directly on the files you are given and reports the accuracy on each data set. I will assume I can run your code if a folder hw4data/ is in the same location .Your code should automatically split the files into train/test or train/dev/test. You should also include clear comments or a separate file that explains what features you decided to use, and what accuracies you obtained for each of the five data sets.\n",
    "\n",
    "(NOTE 1: state-of-the-art for this task ranges between 80% for German nouns and 99% for Spanish verbs)\n",
    "\n",
    "(NOTE 2: if you decide to use a perceptron, be prepared for the possibility that the data set isn't linearly separable. This will depend somewhat on what features you decide to use. For this reason it's a good idea to set a maximum number of iterations for perceptron learning, or use an averaged perceptron with early stopping.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
